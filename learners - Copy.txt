from network_definitions import PolicyNetwork, ValueNetwork, PolicyNetworkConv, ValueNetworkConv
import torch
import torch.optim as optim
import torch.nn as nn
from env import DiceGame, run_episode, update_network
from matplotlib import pyplot as plt
from torch.distributions import Categorical
import gymnasium as gym
from scipy.stats import entropy
import copy
import optuna
from helpers import save_gif
import numpy as np


def reinforce_learner(env, params, device):
    
    ''' 
    Accepts an environment and a parameter dictionary and applies REINFORCE to the provided environment
    Returns: A list of returns for plotting
    '''
    
    episodes = params.get('episodes')
    lr = params.get('lr')
    gamma = params.get('gamma')
    action_space = env.action_space.n
    state_space = env.observation_space.shape[0]
    
    policy_net = PolicyNetwork(state_space, action_space).to(device) #test
    optimizer = optim.Adam(policy_net.parameters(), lr=lr)
    
    reward_totals = []
    
    for n in range(episodes):
        rewards, states, actions, discounted_returns, log_probs = [], [], [], [], []
        done = False
        trunc = False
        
        G = 0
        steps = 0
        
        state, info = env.reset()
        state = torch.from_numpy(state).to(device)
        
        while not done and not trunc:
            action_probs = policy_net(state)
                
            action_dist = Categorical(action_probs)
            action = action_dist.sample()
            next_state, reward, done, trunc, info = env.step(action.item())
            
            log_prob = action_dist.log_prob(action)
            log_probs.append(log_prob)
            
            states.append(next_state)
            actions.append(action)
            rewards.append(reward)
            steps+=1
            state = torch.from_numpy(next_state).to(device)
        
        for r in reversed(rewards):
            G = r + gamma * G
            discounted_returns.insert(0, G)
            
        # Reset the gradients
        optimizer.zero_grad()
            
        # Combine log probabilities and scale by returns
        log_probs = torch.stack(log_probs)
        discounted_returns = torch.tensor(discounted_returns, device=device)
        
        loss = -torch.dot(log_probs, discounted_returns)  # Negative for gradient ascent
        
        # Backpropagate and update the network
        loss.backward()
        optimizer.step()
        
        reward_totals.append(sum(rewards))
        
        if (n+1)%100==0:
            print(f"Episode {n+1}, t = {steps}")
        
    return reward_totals

def a2c_learner(env, params, device):
    
    ''' 
    Description: Accepts an environment and a parameter dictionary and applies
    Advantage Actor Critic (A2C) to the provided environment.
    
    Returns: A list of returns for plotting
    '''
    
    # Get hyperparameter values from the parameter dictionary
    episodes = params.get('episodes')
    lr = params.get('lr')
    gamma = params.get('gamma')
    
    # Define action and state space sizes.
    # State Space --> [Cart Pos, Cart Vel, Pole Pos, Pole Vel]
    # Action Space --> [0 (left), 1 (right)]
    action_space = env.action_space.n
    state_space = env.observation_space.shape[0]
    
    # Create instances of actor (policy) and critic (value) networks, and
    # corresponding optimizers.
    policy_net = PolicyNetwork(state_space, action_space).to(device)
    policy_optimizer = optim.Adam(policy_net.parameters(), lr=lr)
    
    value_net = ValueNetwork(state_space, 1).to(device)
    value_optimizer = optim.Adam(value_net.parameters(), lr=lr)
    
    # Set loss function to be used and init other variables
    mse_loss = nn.MSELoss()
    done, trunc = False, False
    reward_totals = []
    print("\nStarting Training...")
    
    for n in range(episodes):
        
        # Start a new episode
        
        state, info = env.reset()
        steps, rewards = 0, 0
        state = torch.from_numpy(state).to(device)
        finished = False
        
        while not finished:
            # Select an action using the Actor
            action_probs = policy_net(state)
            action_dist = Categorical(action_probs)
            action = action_dist.sample()
            
            # Take the chosen action and observe the next state and reward
            next_state, reward, done, trunc, info = env.step(action.item())
            
            finished = done or trunc
            next_state = torch.from_numpy(next_state).to(device)
            
            # Compute the advantage
            state_value = value_net(state)
            next_state_value = value_net(next_state)
            
            # Use Temporal Difference (TD) target as an estimate for the state-action value
            TD_target = reward + gamma*next_state_value.detach()*(1-finished)
            
            # Calculate advantage
            advantage = reward + gamma*next_state_value.detach()*(1-finished) - state_value

            # Calculate the policy loss (from policy gradient proof). Negate
            # the product for gradient ascent. 
            log_prob = action_dist.log_prob(action)
            policy_loss = -log_prob*advantage.detach()
            
            # Policy --> Zero gradients, backprop, step.
            policy_optimizer.zero_grad()
            policy_loss.backward()
            policy_optimizer.step()
            
            # Calculate the MSE between TD Target and State Value.
            value_loss = mse_loss(TD_target, state_value)
            
            # Value --> Zero gradients, backprop, step.
            value_optimizer.zero_grad()
            value_loss.backward()
            value_optimizer.step()
        
            # Increment counters and set new state
            rewards += reward
            steps += 1
            state = next_state            
            
        reward_totals.append(rewards)
        
        if (n+1)%100==0:
            print(f"Ep: {n+1}, R: {rewards}, policy: {action_probs.cpu().detach().numpy()}, vloss: {value_loss.cpu().detach().numpy()}, ploss: {policy_loss.cpu().detach().numpy()}")
    
    print("Finished!\n")    
    return reward_totals

def ppo_learner(env, params, device, trial = False):
    
    ''' 
    Description: Accepts an environment and a parameter dictionary and applies
    Advantage Actor Critic (A2C) to the provided environment.
    
    Returns: A list of returns for plotting
    '''
    
    # Get hyperparameter values from the parameter dictionary
    iterations = params.get('iterations')
    lr = params.get('lr')
    gamma = params.get('gamma')
    T = params.get('rollout_len')
    eps = params.get('eps')
    
    # Define action and state space sizes.
    action_space = env.action_space.n
    state_space = env.observation_space.shape[0]
    
    # Create instances of actor (policy) and critic (value) networks, and corresponding optimizers.
    policy_net = PolicyNetwork(state_space, action_space).to(device)
    old_policy_net = copy.deepcopy(policy_net)
    policy_optimizer = optim.Adam(policy_net.parameters(), lr=lr)
    
    value_net = ValueNetwork(state_space, 1).to(device)
    value_optimizer = optim.Adam(value_net.parameters(), lr=lr)
    
    # Set loss function to be used and init other variables
    mse_loss = nn.MSELoss()
    reward_totals = []
    global_steps = 0
    
    print("\nStarting Training...")
    state, info = env.reset()
    state = torch.from_numpy(state).to(device)
    
    for n in range(iterations):
        
        buffer = perform_rollout(env, old_policy_net, value_net, T, device) # (50, )
        
        #return_to_go = get_returns(buffer["reward"], gamma) # (50, ) OLD
        return_to_go = get_returns(buffer["reward"], buffer["dones"], gamma) # NEW
        
        rollout_states = torch.stack(buffer["state"]).to(device)
        rollout_returns = torch.tensor(return_to_go).to(device)
        rollout_actions = torch.tensor(buffer["action"]).to(device)
        rollout_values = torch.tensor(buffer["values"]).to(device)
        rollout_dones = torch.tensor(buffer["dones"]).to(device)
        old_log_probs = torch.tensor(buffer["old_log_probs"]).to(device)
        rollout_rewards = torch.tensor(buffer["reward"]).to(device)
        
        with torch.no_grad():
            next_value = value_net(torch.tensor(buffer['state'][-1]).to(device))
        
        advantage = rollout_returns - rollout_values
        advantage, rollout_returns = compute_gae(rollout_rewards, rollout_dones, rollout_values, next_value, gamma)
        rollout_returns = rollout_returns.to(device)
        advantage = (advantage - advantage.mean()) / (advantage.std() + 1e-8)
        
        action_probs = policy_net(rollout_states)
        action_dist = Categorical(action_probs)
        new_log_probs = action_dist.log_prob(rollout_actions)
             
        ratio = torch.exp(new_log_probs - old_log_probs)
        noclip_term = advantage*ratio
        clip_term = torch.clamp(ratio, min=1-eps, max=1+eps)*advantage
        ppo_loss = -torch.min(noclip_term, clip_term).mean()
        
        policy_optimizer.zero_grad()
        ppo_loss.backward()
        policy_optimizer.step()
        
        
        training_values = value_net(rollout_states).squeeze()
        value_loss = mse_loss(training_values, rollout_returns)

        value_optimizer.zero_grad()
        value_loss.backward()
        value_optimizer.step()
        
        old_policy_net = copy.deepcopy(policy_net)
            
        divider = round(0.01*iterations)
        
        if n==0 or n==iterations-1 or (n+1)%divider==0:
            eval_return, eval_num = evaluate_policy(env, policy_net, device, num_episodes=5)
            reward_totals.append(eval_return)
            if trial:
                trial.report(eval_return, step=n)
                if trial.should_prune():
                    raise optuna.TrialPruned()
            print(f"Iter: {n+1}, R: {eval_return}")
            
    
    print("Finished!\n")    
    return reward_totals

def ppo_learner_image(env, params, device, trial = False):
    
    ''' 
    Description: Accepts an environment and a parameter dictionary and applies
    Advantage Actor Critic (A2C) to the provided environment.
    
    Returns: A list of returns for plotting
    '''
    
    # Get hyperparameter values from the parameter dictionary
    iterations = params.get('iterations')
    lr = params.get('lr')
    gamma = params.get('gamma')
    T = params.get('rollout_len')
    eps = params.get('eps')
    entropy_coef = params.get("entropy")
    
    # Define action and state space sizes.
    # State Space --> [Cart Pos, Cart Vel, Pole Pos, Pole Vel]
    # Action Space --> [0 (left), 1 (right)]
    action_space = env.action_space.n
    state_space = np.roll(np.asarray(env.observation_space.shape), 1)
    # Create instances of actor (policy) and critic (value) networks, and
    # corresponding optimizers.
    policy_net = PolicyNetworkConv(state_space, action_space).to(device)
    old_policy_net = copy.deepcopy(policy_net)
    policy_optimizer = optim.Adam(policy_net.parameters(), lr=lr)
    
    value_net = ValueNetworkConv(state_space, 1).to(device)
    value_optimizer = optim.Adam(value_net.parameters(), lr=lr)
    
    # Set loss function to be used and init other variables
    mse_loss = nn.MSELoss()
    reward_totals = []
    global_steps = 0
    max_eval = -1
    max_index = 0
    print("\nStarting Training...")
    state, info = env.reset()
    
    state = torch.from_numpy(state).float().movedim(-1, 0).unsqueeze(0).to(device)
    #print(f"Initial State dtype: {state.dtype}, Shape: {state.shape}")
    for n in range(iterations):
        
        buffer = perform_rollout(env, old_policy_net, value_net, T, device) # (50, )
        #print(buffer["reward"])
        return_to_go = get_returns(buffer["reward"], buffer["dones"], gamma) # NEW)
        
        rollout_states = torch.stack(buffer["state"]).float().to(device)
        rollout_returns = torch.tensor(return_to_go).to(device)
        rollout_actions = torch.tensor(buffer["action"]).to(device)
        rollout_values = torch.tensor(buffer["values"]).to(device)
        old_log_probs = torch.tensor(buffer["old_log_probs"]).to(device)
        
        advantage = rollout_returns - rollout_values
        
        
        advantage = (advantage - advantage.mean()) / (advantage.std() + 1e-8)
        #print(f"Passing rollout Data into Network - Rollout Shape: {rollout_states.shape}")
        action_probs = policy_net(rollout_states.movedim(-1, 1))
        action_dist = Categorical(action_probs)
        new_log_probs = action_dist.log_prob(rollout_actions)
             
        ratio = torch.exp(new_log_probs - old_log_probs)
        noclip_term = advantage*ratio
        clip_term = torch.clamp(ratio, min=1-eps, max=1+eps)*advantage
        entropyt = action_dist.entropy()
        ppo_loss = -torch.min(noclip_term, clip_term).mean() - entropy_coef*entropyt.mean()
        
        policy_optimizer.zero_grad()
        ppo_loss.backward()
        policy_optimizer.step()
        
        
        training_values = value_net(rollout_states.movedim(-1, 1)).squeeze()

        value_loss = mse_loss(training_values, rollout_returns.float())
        print(f"Training Shape {training_values.shape}, Training Sum {training_values.sum()}, Rollout Shape {rollout_returns.float().shape}, Rollout Sum {rollout_returns.float().sum()}")
        print()
        print("Rollouts...)")
        print()
        #print(f"Loss Datatype: {value_loss.dtype}")
        value_optimizer.zero_grad()
        value_loss.backward()
        value_optimizer.step()
        
        old_policy_net = copy.deepcopy(policy_net)
            
        divider = round(0.01*iterations)
        
        if n==0 or n==iterations-1 or (n+1)%divider==0:
            eval_return, ep_length = evaluate_policy(env, policy_net, device, num_episodes=5)
            if eval_return > max_eval:
                max_eval = eval_return
                max_index = n
            reward_totals.append(eval_return)
            if trial:
                trial.report(eval_return, step=n)
                if trial.should_prune():
                    raise optuna.TrialPruned()
            print(f"Iter: {n+1}, R: {eval_return}, V: {value_loss}, P: {ppo_loss}, EpLen: {ep_length}, Last Action: {rollout_actions[-1]}")
    #print(rollout_states.shape)     
    #save_gif(rollout_states[max_index, :].char(), "demo.gif")
    print("Finished!\n")    
    return reward_totals

def get_returns(rewards, dones, gamma=0.99):
    returns = []
    R = 0
    for reward, done in zip(reversed(rewards), reversed(dones)):
        if done:
            R = 0
        else:
            R = reward + gamma * R
        returns.insert(0, R)
    return returns

def select_action(policy, state):
    action_probs = policy(state)
    
    action_dist = Categorical(action_probs)
    action = action_dist.sample()
    log_prob = action_dist.log_prob(action)
    return action, log_prob

def perform_rollout(env, actor, critic, T, device):
    buffer = {
        "state": [],
        "action": [],
        "reward": [],
        "dones": [],
        "old_log_probs": [],
        "values": []
        }
    
    state, info = env.reset()
    state = torch.from_numpy(state).float().to(device)
    for _ in range(T):
        with torch.no_grad():
            action, log_prob = select_action(actor, state.movedim(-1, 0).unsqueeze(0))
            value = critic(state.movedim(-1, 0).unsqueeze(0))
            
        next_state, reward, done, trunc, info = env.step(action.item())
        next_state = torch.from_numpy(next_state).float().to(device)
        
        buffer["state"].append(state)
        buffer["action"].append(action)
        buffer["reward"].append(reward)
        buffer["dones"].append(done or trunc)
        buffer["old_log_probs"].append(log_prob)
        buffer["values"].append(value)
        
        state = next_state.float()
        
        if done or trunc:
            state, info = env.reset()
            state = torch.from_numpy(state).float().to(device)
            
    return buffer
    
def evaluate_policy(env, policy_net, device, num_episodes=5):
    returns = []
    #print("Evaluating Current Policy")
    for num in range(num_episodes):
        state, info = env.reset()
        states = [state]
        done = False
        trunc = False
        ep_return = 0
        while not (done or trunc):
            with torch.no_grad():
                state_tensor = torch.tensor(state).movedim(-1, 0).unsqueeze(0).float().to(device)
                action_probs = policy_net(state_tensor)
                action = torch.argmax(action_probs, dim=-1).item()
            state, reward, done, trunc, info = env.step(action)
            #print(f"{num}: {reward}")
            states.append(state)
            
            ep_return += reward
        returns.append(ep_return)
    return sum(returns) / len(returns), len(states)

def compute_gae(
    rewards: torch.Tensor,
    dones: torch.Tensor,
    values: torch.Tensor,
    next_value: torch.Tensor,
    gamma: float = 0.99,
    lam: float = 0.95
) -> (torch.Tensor, torch.Tensor):
    """
    Compute GAE advantages and returns.

    Args:
        rewards:  Tensor of shape (T,) or (T, N)
        dones:    Tensor of same shape; 1.0 if episode ended, else 0.0
        values:   Tensor of shape (T,) or (T, N)
        next_value: Tensor of shape (N,) or scalar: V(s_{T})
        gamma:    discount factor
        lam:      GAE smoothing parameter

    Returns:
        advantages: Tensor same shape as rewards/values
        returns:    Tensor same shape as rewards/values
    """
    T = rewards.size(0)
    # buffer for advantages
    
    advantages = torch.zeros_like(rewards)
    # start GAE accumulator at zero (shape = batch‐shape)
    gae = torch.zeros_like(next_value)

    # append V(s_{T}) so we can always look “one step ahead”
    #next_value = next_value.unsqueeze(0)          # shape (1, N) or (1,)
    values = torch.cat([values, next_value], dim=0)

    for t in reversed(range(T)):
        # mask = 0 if done, 1 otherwise
        mymask = (~dones[t]).float()
        # TD error δ_t = r_t + γ·V_{t+1}·mask − V_t
        delta = rewards[t] + gamma * values[t + 1] * mymask - values[t]
        # GAE recursion
        gae = delta + gamma * lam * mymask * gae
        advantages[t] = gae

    # compute discounted returns R_t = A_t + V_t
    returns = advantages + values[:-1]
    return advantages, returns
        